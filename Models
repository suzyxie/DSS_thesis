"""##Teacher Model"""

pip install segmentation-models

#resize the images and masks
import os
import cv2

# Define the target size
target_size = (720, 720)

# Define the directories where your images and masks are located
sample_images_dir = "/content/drive/MyDrive/sample dataset/images "
sample_masks_dir = "/content/drive/MyDrive/sample dataset/masks"

# Create a directory to save the resized images and masks
resized_images_dir = "/content/drive/MyDrive/sample dataset/resized_images"
resized_masks_dir = "/content/drive/MyDrive/sample dataset/resized_masks"

# Create the directories if they don't exist
os.makedirs(resized_images_dir, exist_ok=True)
os.makedirs(resized_masks_dir, exist_ok=True)

# List all files in the images directory
image_files = os.listdir(sample_images_dir)

# Iterate through the images and resize them
for image_file in image_files:
    # Load the image
    image_path = os.path.join(sample_images_dir, image_file)
    image = cv2.imread(image_path)

    # Resize the image to the target size
    resized_image = cv2.resize(image, target_size)

    # Save the resized image
    resized_image_path = os.path.join(resized_images_dir, image_file)
    cv2.imwrite(resized_image_path, resized_image)

    # Process the corresponding mask
    mask_file = image_file.replace('.jpg', '_lab.png')  # Assuming masks have the same name as images with a .png extension
    mask_path = os.path.join(sample_masks_dir, mask_file)

    # Load the mask
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    # Resize the mask to the target size
    resized_mask = cv2.resize(mask, target_size)

    # Save the resized mask
    resized_mask_path = os.path.join(resized_masks_dir, mask_file)
    cv2.imwrite(resized_mask_path, resized_mask)

# Define the target data range for normalization (e.g., [0, 1])
min_value = 0
max_value = 1

# Define a function to normalize images in a directory
def normalize_images_in_directory(directory):
    for filename in os.listdir(directory):
        if filename.endswith('.jpg'):
            # Load an image
            img_path = os.path.join(directory, filename)
            img = cv2.imread(img_path)

            if img is not None:
                # Normalize the image to the specified range
                img_normalized = (img - img.min()) / (img.max() - img.min()) * (max_value - min_value) + min_value

                # Save the normalized image back to the directory
                normalized_img_path = os.path.join(directory, filename)
                cv2.imwrite(normalized_img_path, img_normalized * 255)  # Scale back to [0, 255] for saving as image

# Call the function to normalize images in each directory
normalize_images_in_directory(resized_images_dir)

train_images_720 ="/content/drive/MyDrive/Resized_data_720/resized_train/images"
train_masks_720= "/content/drive/MyDrive/Resized_data_720/resized_train/masks"

train_data = []
train_labels = []

for filename in os.listdir(train_images_720):
    if filename.endswith(".jpg"):  # Assuming your images are in JPEG format
        # Load the image
        image = cv2.imread(os.path.join(train_images_720, filename))
        train_data.append(image)

        # Load the corresponding mask
        mask_filename = os.path.join(train_masks_720, filename.replace(".jpg", "_lab.png"))
        mask = cv2.imread(mask_filename, cv2.IMREAD_GRAYSCALE)
        train_labels.append(mask)

X_train_720 = np.array(train_data)
y_train_720 = np.array(train_labels)
np.save('/content/drive/MyDrive/Resized_data_720/X_train_720.npy', X_train_720)
np.save('/content/drive/MyDrive/Resized_data_720/y_train_720.npy', y_train_720)

val_images_720 = "/content/drive/MyDrive/Resized_data_720/resized_val/images"
val_masks_720 = "/content/drive/MyDrive/Resized_data_720/resized_val/masks"

val_data = []
val_labels = []

for filename in os.listdir(val_images_720):
    if filename.endswith(".jpg"):
        # Load the image
        image = cv2.imread(os.path.join(val_images_720, filename))
        val_data.append(image)

        # Load the corresponding mask
        mask_filename = os.path.join(val_masks_720, filename.replace(".jpg", "_lab.png"))
        mask = cv2.imread(mask_filename, cv2.IMREAD_GRAYSCALE)
        val_labels.append(mask)

X_val_720 = np.array(val_data)
y_val_720 = np.array(val_labels)
np.save('/content/drive/MyDrive/Resized_data_720/X_val_720.npy', X_val_720)
np.save('/content/drive/MyDrive/Resized_data_720/y_val_720.npy', y_val_720)

test_images_720 = "/content/drive/MyDrive/Resized_data_720/resized_test/images"
test_masks_720 = "/content/drive/MyDrive/Resized_data_720/resized_test/masks"

test_data = []
test_labels = []

for filename in os.listdir(test_images_720):
    if filename.endswith(".jpg"):
        # Load the image
        image = cv2.imread(os.path.join(test_images_720, filename))
        test_data.append(image)

        # Load the corresponding mask
        mask_filename = os.path.join(test_masks_720, filename.replace(".jpg", "_lab.png"))
        mask = cv2.imread(mask_filename, cv2.IMREAD_GRAYSCALE)
        test_labels.append(mask)

X_test_720 = np.array(test_data)
y_test_720 = np.array(test_labels)
np.save('/content/drive/MyDrive/Resized_data_720/X_test_720.npy', X_test_720)
np.save('/content/drive/MyDrive/Resized_data_720/y_test_720.npy', y_test_720)

X_train = np.load('/content/drive/MyDrive/Resized_data_720/X_train_720.npy')
y_train = np.load('/content/drive/MyDrive/Resized_data_720/y_train_720.npy')
X_val = np.load('/content/drive/MyDrive/Resized_data_720/X_val_720.npy')
y_val = np.load("/content/drive/MyDrive/Resized_data_720/y_val_720.npy")

num_classes = 7
y_train_reshaped = np.zeros((y_train.shape[0], y_train.shape[1], y_train.shape[2], num_classes), dtype=np.uint8)

# Iterate over each class and create the respective channel
for class_idx in range(num_classes):
    y_train_reshaped[:, :, :, class_idx] = (y_train == class_idx).astype(np.uint8)

y_val_reshaped = np.zeros((y_val.shape[0], y_val.shape[1], y_val.shape[2], num_classes), dtype=np.uint8)

# Iterate over each class and create the respective channel
for class_idx in range(num_classes):
    y_val_reshaped[:, :, :, class_idx] = (y_val == class_idx).astype(np.uint8)

X_train = X_train.astype('float32')
y_train = y_train_reshaped.astype('float32')
X_val = X_val.astype('float32')
y_val = y_val_reshaped.astype('float32')

# PSPNet teacher model
import os
os.environ["SM_FRAMEWORK"] = "tf.keras"

from tensorflow import keras
import segmentation_models as sm
from segmentation_models import PSPNet,Linknet



teacher_model = PSPNet(backbone_name='resnet101',
                       classes= 7, input_shape=(720, 720, 3),
                       activation='softmax',
                       encoder_weights='imagenet',
                       downsample_factor= 8,
                       psp_conv_filters = 128,
                       psp_pooling_type='avg',
                       psp_dropout=None
                       )

teacher_model.summary()

"""##Final teacher model scripts and results are saved locally"""

import tensorflow as tf
from tensorflow.keras.optimizers.schedules import PolynomialDecay
from tensorflow.keras.losses import CategoricalFocalCrossentropy
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.metrics import OneHotMeanIoU

# Define your learning rate schedule with the given parameters
base_learning_rate = 0.0001
momentum = 0.9
weight_decay = 0.0001
power = 0.9
auxiliary_loss_weight = 0.4

# Calculate the total number of training steps (you need to adjust this based on your dataset and batch size)
total_steps = 10000

learning_rate_schedule = PolynomialDecay(
    initial_learning_rate=base_learning_rate,
    decay_steps=total_steps,
    end_learning_rate=0,  # You can set this to 0 or any other final learning rate you desire
    power=power
)

# Create the legacy Adam optimizer with the specified momentum and weight decay
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule, beta_1=momentum, beta_2=0.999, epsilon=1e-7, weight_decay=weight_decay)

# Compile your model using the custom optimizer and loss
teacher_model.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalFocalCrossentropy(),
                      metrics=[tf.keras.metrics.OneHotMeanIoU(num_classes=num_classes)])

history = teacher_model.fit(X_train, y_train,
                            validation_data=(X_val, y_val),
                            epochs= 100
                            )

from tensorflow.keras.models import load_model

teacher_model_trained = load_model("/content/drive/MyDrive/teacher_model_FCloss2_150epochs.keras")

"""## Student Models"""


student_model1= PSPNet(backbone_name='mobilenet',
                       classes= 7, input_shape=(720, 720, 3),
                       activation='softmax',
                       encoder_weights= None,
                       downsample_factor= 4,
                       psp_conv_filters = 16,
                       psp_pooling_type='avg',
                       psp_dropout=None,
                       psp_use_batchnorm=False,
                       )

student_model1. summary()

"""##Knowledge Distillation"""

class Distiller(keras.Model):
    def __init__(self, student, teacher):
        super().__init__()
        self.teacher = teacher
        self.student = student

    def compile(
        self,
        optimizer,
        metrics,
        student_loss_fn,
        distillation_loss_fn,
        alpha=0.1,
        temperature=3,
    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super().compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha
        self.temperature = temperature

    def train_step(self, data):
        # Unpack data
        x, y = data

        # Forward pass of teacher
        teacher_predictions = self.teacher(x, training=False)

        with tf.GradientTape() as tape:
            # Forward pass of student
            student_predictions = self.student(x, training=True)

            # Compute losses
            student_loss = self.student_loss_fn(y, student_predictions)

            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531
            # The magnitudes of the gradients produced by the soft targets scale
            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.
            distillation_loss = (
                self.distillation_loss_fn(
                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),
                    tf.nn.softmax(student_predictions / self.temperature, axis=1),
                )
                * self.temperature**2
            )

            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss

        # Compute gradients
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, student_predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update(
            {"student_loss": student_loss, "distillation_loss": distillation_loss}
        )
        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        y_prediction = self.student(x, training=False)

        # Calculate the loss
        student_loss = self.student_loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss})
        return results

num_classes =7
base_learning_rate = 0.0001
momentum = 0.9
weight_decay = 0.0001
power = 0.9
auxiliary_loss_weight = 0.4

# Calculate the total number of training steps (you need to adjust this based on your dataset and batch size)
total_steps = 10000

learning_rate_schedule = PolynomialDecay(
    initial_learning_rate=base_learning_rate,
    decay_steps=total_steps,
    end_learning_rate=0,  # You can set this to 0 or any other final learning rate you desire
    power=power
)

# Create the legacy Adam optimizer with the specified momentum and weight decay
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule, beta_1=momentum, beta_2=0.999, epsilon=1e-7, weight_decay=weight_decay)
distiller = Distiller(student=student_model1, teacher=teacher_model_trained )
distiller.compile(
    optimizer=optimizer,
    metrics=[tf.keras.metrics.OneHotMeanIoU(num_classes=num_classes)],
    student_loss_fn=tf.keras.losses.CategoricalFocalCrossentropy(),
    distillation_loss_fn= tf.keras.losses.KLDivergence(),
    alpha=0.1,
    temperature=10
)

distiller.fit(X_train, y_train, epochs=10)

